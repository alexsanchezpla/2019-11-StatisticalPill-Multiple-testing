<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
  <head>
    <title>.</title>
    <meta charset="utf-8" />
    <meta name="author" content="." />
    
    <link rel="stylesheet" href="mycss.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# .
### .

---


&lt;!-- edit name1 and name2 in the YAML above --&gt;




&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

# Statistics and Bioinformatics Unit (UEB)

&lt;img src="images/uebvhir.png" width="100%" style="display: block; margin: auto;" /&gt;

.center[
.font150[
[http://ueb.vhir.org](http://ueb.vhir.org)
]
]
---

# Outline of the talk

- Why this pill. Motivation &amp; Cases
- From Type I errors to Multiple Error Rates
- Strategies for Multiple Testing Adjustments
- Multiple testing adjustment in practice
- Variations on a theme
- Should we correct or not? When?
- Recomendations (guidelines)
- Summary


---
# When multiplicity is ignored ...

(Bad management) of multiplicity can yield potentially spurious results

&lt;img src="images/multiplicityComic1.png" width="90%" style="display: block; margin: auto;" /&gt;

http://xkcd.com/882/

---

# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic1b.png" width="90%" style="display: block; margin: auto;" /&gt;
---

# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic1c.png" width="90%" style="display: block; margin: auto;" /&gt;
---

# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic1d.png" width="90%" style="display: block; margin: auto;" /&gt;
---

# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic1e.png" width="90%" style="display: block; margin: auto;" /&gt;
---


# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic3.png" width="50%" style="display: block; margin: auto;" /&gt;
---
# But, what is multiplicity?

- The __multiple comparisons__, __multiplicity__ or __multiple testing__ problem occurs when one considers a set of statistical inferences simultaneously ... 

- The more inferences are made, the more likely erroneous inferences are to occur.

- Multiplicity appears in many distinct situations

---

# Example: __*Multiple outcomes*__

### Frantic paresis association with distinct outcomes

&lt;img src="images/multiplicityExamples1MultipleOutcomes.png" width="100%" style="display: block; margin: auto;" /&gt;
---

# Example: __*Several groups*__

### Raw p-values of post-Hoc (after ANOVA) pairwise comparisons

&lt;br&gt;

|  | B | C | D |  |
|---|-------|-------|-------|---|
|__A__ | 0.045 | 0.098 | 0.062 |  |
|__B__ |  | 0.683 | 0.891 |  |
|__C__ |  |  | 0.638 |  |

---

# Example: __*Omics data*__

### A simple microarray analysis yields tables with thousands of test results

&lt;img src="images/multiplicityExamples3Microarrays.png" width="100%" style="display: block; margin: auto;" /&gt;
---


# Hypothesis Testing Refresher

- Most situations desbribed above can be described or related with a __test of hypothesis__.
- Tests use to be summarized with __p-values__. &lt;br&gt;
- __p-value__ : Probability, assuming no effect `\((H_0)\)`, to obtain a difference greater or equal than the one observed on a given sample. &lt;br&gt;
- Standard criterion: "reject `\(H_0\)` if `\(p \geq \alpha\)`".
     
---

# Decision table and error types

- When decisions are made, based on data, one can take right or wrong decisions
- Wrong decisions: __type I__ or __type II errors__.


&lt;img src="images/decisionTable1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Controlling (type I) Errors

- A test is said to _control type I error_ if the probability of wrongly rejecting `\(H_0\)` is smaller than the significance level of the test.
.font80[
$$
    P[\mbox{Reject} \, H_0 | H_0 \, \mbox{true}]=P[FP] \leq \alpha
$$
]
- This does not guarantee anything on the power of the test.

    + A test can control type I error while having small power


---

# From 1 to `\(&gt;1\)` hypotheses

- As more hypothesis are tested simultaneously, 
the probability of wrongly rejecting __at least one true null__ increases:

.font80[

- P(Making 1 error) = `\(\alpha\)`
    
- P(__Not__ Making 1 error) = `\(1-\alpha\)`
    
- P(__Not__ Making 1 error in 2 tests) = `\((1-\alpha)^2\)`
  
- 3, 4, ..., m tests
    
- P(__Not__ Making 1 error in `\(m\)` tests) = `\((1-\alpha)^m\)`
    
- P(Making __at least__ 1 error in `\(m\)` tests) = `\(1-(1-\alpha)^m\)`
]
---

# From 1 to `\(&gt;1\)` hypotheses


.pull-left[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; m &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Prob &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.05000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.09750 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.18549 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.33658 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.45964 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.99408 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.00000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]
.pull-right[
![](Multiple_testing_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]
---

# Implications for our examples

- If we test multiple hypothesis simultaneously the overall type I error probability is not controlled anymore.

- Testing 12 tests simultaneosly yields almost a 50% chance of  a statistically significant result  __even if none of the hypothesis tested is false __

- How do we incorporate the impact of multiple testing on our inference?

---

# A simulated example (1)

- We simulated an omics study with 6000 genes whose expression has been measured on 8 cases and 8 controls, and where __no gene shows real difference between them__.

- What happens if we call _differentially expressed_ any gene with `\(p &lt; 0.05\)`

- The number of genes falsely rejected will be on average of `\(6000 \times \alpha\)`.
    
---

# We start with no differences ...

&lt;img src="images/simulatedArray1.png" width="120%" style="display: block; margin: auto;" /&gt;
---

# As more genes are checked ...

&lt;img src="images/simulatedArray2.png" width="110%" style="display: block; margin: auto;" /&gt;
---

# All together, sorted by p-values

&lt;img src="images/simulatedArray3.png" width="110%" style="display: block; margin: auto;" /&gt;
---

# So what can be done?

- Intuitive idea: Doing many tests increases the chances of calling false positives,

- This may be compensated 
    + Using _more restrictive error rates_, for instance 0.01 or 0.005 instead of 0.05.
    
    + Adjusting ("correcting") the p-values to compensate for the number of tests.
    
---

# Distinct Error Rates

- Individual error rate (IER)
    + Error rate of a single test.
    + For a test with 5% significance level the IER is 0.05
    
- Global Error Rate
    + Error rate for one or several groups of tests. 
    + For a group of tests each with 5% significance the global error rate is &gt; 5%

---

&lt;!-- # Extending type I error control --&gt;

&lt;!-- - With more than 1 test rejecting hypotheses with a p-value less than `\(\alpha\)` doesn’t control for `\(P[FD]\)` anymore. --&gt;
&lt;!-- - What can be done? --&gt;
&lt;!--      + Extend the idea of type I error --&gt;
&lt;!--           - FWER and FDR are two such extensions --&gt;

&lt;!--      + Look for procedures that control the probability for these extended error types --&gt;
&lt;!--           - Mainly adjust raw p-values --&gt;

&lt;!-- --- --&gt;


# Decision table for many tests

- With many tests we count discoveries 

&lt;img src="images/decisionTable2.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Two main error rate extensions

- Family Wise Error Rate (FWER) 
    + FWER is probability of at least 1 False Discovery
    + FWER = P (FD &gt; 0) 
   
- False Discovery Rate (FDR)
    + FDR is expected value of proportion of False Discoveries among all Discoveries .
    + FDR = E (FD/D; D&gt;0)
---

# FWER / FDR control procedures

- FWER 
    + Bonferroni
    + Holm (1979)
    + Hochberg (1986)

- FDR
    + Benjamini &amp; Hochberg (1995)
    + Benjamini &amp; Yekutieli (2001)
---

# Controlling the FWER (Bonferroni)

- Bonferroni procedure: Adjust significance level for number of tests performed (m)

  + Use significance level `\(\alpha/m\)`, 

- Equivalently, adjust p-values multiplying all p-values by m.

- Other, more efficient procedures available: See a statistician

---

# Example. Presenting data.

.pull-left[
.font70[

- García-Arenzana et al. (2014) tested associations of 25 dietary variables with mammographic density, an important risk factor for breast cancer, in Spanish women.
- They found the following results (only first 10 are shown)
]
]
.pull-right[
&lt;img src="images/dietExample1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

[See complete example](http://www.biostathandbook.com/multiplecomparisons.html)

---

# Example. Bonferroni (FWER)

&lt;img src="images/dietExample2.png" width="100%" style="display: block; margin: auto;" /&gt;
---


# Controlling the FDR (B &amp; H)

- Benjamini-Hochberg procedure: Provides control of FDR for a fixed FDR value 
    + 5% FDR: On average, 5% of your significant findings will be false
    
- Important: FDR is not an individual error rate.
    + A number higher than 0.05, such as 0.10 or 0.25 can be used

---

# Benjamini &amp; Hochberg
    
- Procedure is relatively simple
    + Order the p-values
    + To provide control at a `\(Q\)` FDR value compare i-th smallest p-value to  `\(i \times Q/m\)`

- Instead of setting the FDR at a fixed value and establishing significance/non significance an, __adjusted p-value__ may be computed.

---

# Example. B-H (FDR)

&lt;img src="images/dietExample3.png" width="100%" style="display: block; margin: auto;" /&gt;
---


# What error rate to control for

- FWER Controls for no (0) false positives
     + Rejects many fewer hypothese (less false positives),
     + but you are likely to miss many.
     + Adequate if goal is to identify few cases that differ between two groups.
     
---

# What error rate to control for

- FDR Controls the (expected) proportion of false positives
    + if you can tolerate more false positives
    + you will get many fewer false negatives
    + Adequate if goal is to pursue the study e.g. to determine functional relationships among genes


---

# Philosophical issues: To adjust or not.

- Differing opinions on whether or when adjustment is needed
- Choice of family of hypotheses to adjust for is arbitrary, and results are very sensitive to this choice
    + Choose small, more focused families, specified a priori (in writing) to avoid cheating.
- Increase in type II errors due to adjustment (loss of power or ability to detect real differences)
    + Use an appropriate and powerful testing procedure (see yor bedside statistician).
- Argue for unadjusted analysis if required, but with full disclosure of data analysis procedures
    + Difficult for reviewers to evaluate.
    
---
# Philosophical issues: To adjust or not.

- Need for adjustment, and best method of adjustment, is often scenario dependent
- Things to consider (Westfall et al., 1999)
    + Is it plausible that many of the null hypotheses might be true?
    + Do you want to ensure reproducibility, or be able to claim that an identified significant finding is in fact real?
    + Do you want to heavily mine the data to find a “significant” result?
    + Is your study expensive and unlikely to be repeated before serious actions are taken?
    + Is there an important cost associated with type I errors?

---

# References

---

# QUESTIONS?

&lt;img src="images/questions.png" width="90%" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
