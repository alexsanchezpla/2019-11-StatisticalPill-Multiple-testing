<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>.</title>
    <meta charset="utf-8" />
    <meta name="author" content="." />
    
    <link rel="stylesheet" href="mycss.css" type="text/css" />
  </head>
  <body>
    <textarea id="source">
class: center, middle, inverse, title-slide

# .
### .

---


&lt;!-- edit name1 and name2 in the YAML above --&gt;




&lt;style type="text/css"&gt;
.remark-slide-content {
    font-size: 30px;
    padding: 1em 4em 1em 4em;
}
&lt;/style&gt;

# Statistics and Bioinformatics Unit (UEB)

&lt;img src="images/uebvhir.png" width="100%" style="display: block; margin: auto;" /&gt;

.center[
.font150[
[http://ueb.vhir.org](http://ueb.vhir.org)
]
]
---

# Outline of the talk

- Why this pill. Motivation &amp; Cases
- From Type I errors to Multiple Error Rates
- Strategies for Multiple Testing Adjustments
- Multiple testing adjustment in practice
- Variations on a theme
- Should we correct or not? When?
- Recomendations (guidelines)
- Summary


---
# When multiplicity is ignored ...

(Bad management) of multiplicity can yield potentially spurious results

&lt;img src="images/multiplicityComic1.png" width="90%" style="display: block; margin: auto;" /&gt;

http://xkcd.com/882/

---

# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic1b.png" width="90%" style="display: block; margin: auto;" /&gt;
---

# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic1c.png" width="90%" style="display: block; margin: auto;" /&gt;
---

# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic1d.png" width="90%" style="display: block; margin: auto;" /&gt;
---

# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic1e.png" width="90%" style="display: block; margin: auto;" /&gt;
---


# When multiplicity is ignored ...

&lt;img src="images/multiplicityComic3.png" width="50%" style="display: block; margin: auto;" /&gt;
---
# But, what is multiplicity?

- The __multiple comparisons__, __multiplicity__ or __multiple testing__ problem occurs when one considers a set of statistical inferences simultaneously ... 

- The more inferences are made, the more likely erroneous inferences are to occur.

- Multiplicity appears in many distinct situations

---

# Example: __*Multiple outcomes*__

### Frantic paresis association with distinct outcomes

&lt;img src="images/multiplicityExamples1MultipleOutcomes.png" width="100%" style="display: block; margin: auto;" /&gt;
---

# Example: __*Several groups*__

### Raw p-values of post-Hoc (after ANOVA) pairwise comparisons

&lt;br&gt;

|  | B | C | D |  |
|---|-------|-------|-------|---|
|__A__ | 0.045 | 0.098 | 0.062 |  |
|__B__ |  | 0.683 | 0.891 |  |
|__C__ |  |  | 0.638 |  |

---

# Example: __*Omics data*__

### A simple microarray analysis yields tables with thousands of test results

&lt;img src="images/multiplicityExamples3Microarrays.png" width="100%" style="display: block; margin: auto;" /&gt;
---


# Hypothesis Testing Refresher

- Most situations desbribed above can be described or related with a __test of hypothesis__.
- Tests use to be summarized with __p-values__. &lt;br&gt;
- __p-value__ : Probability, assuming no effect `\((H_0)\)`, to obtain a difference greater or equal than the one observed on a given sample. &lt;br&gt;
- Standard criterion: "reject `\(H_0\)` if `\(p \geq \alpha\)`".
     
---

# Decision table and error types

- When decisions are made, based on data, one can take right or wrong decisions
- Wrong decisions: __type I__ or __type II errors__.


&lt;img src="images/decisionTable1.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Controlling (type I) Errors

- A test is said to _control type I error_ if the probability of wrongly rejecting `\(H_0\)` is smaller than the significance level of the test.
.font80[
$$
    P[\mbox{Reject} \, H_0 | H_0 \, \mbox{true}]=P[FP] \leq \alpha
$$
]
- This does not guarantee anything on the power of the test.

    + A test can control type I error while having small power


---

# From 1 to `\(&gt;1\)` hypotheses

- As more hypothesis are tested simultaneously, 
the probability of wrongly rejecting __at least one true null__ increases:

.font80[

- P(Making 1 error) = `\(\alpha\)`
    
- P(__Not__ Making 1 error) = `\(1-\alpha\)`
    
- P(__Not__ Making 1 error in 2 tests) = `\((1-\alpha)^2\)`
  
- 3, 4, ..., m tests
    
- P(__Not__ Making 1 error in `\(m\)` tests) = `\((1-\alpha)^m\)`
    
- P(Making __at least__ 1 error in `\(m\)` tests) = `\(1-(1-\alpha)^m\)`
]
---

# From 1 to `\(&gt;1\)` hypotheses


.pull-left[
&lt;table&gt;
 &lt;thead&gt;
  &lt;tr&gt;
   &lt;th style="text-align:right;"&gt; m &lt;/th&gt;
   &lt;th style="text-align:right;"&gt; Prob &lt;/th&gt;
  &lt;/tr&gt;
 &lt;/thead&gt;
&lt;tbody&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.05000 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 2 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.09750 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 4 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.18549 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 8 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.33658 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 12 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.45964 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 100 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 0.99408 &lt;/td&gt;
  &lt;/tr&gt;
  &lt;tr&gt;
   &lt;td style="text-align:right;"&gt; 1000 &lt;/td&gt;
   &lt;td style="text-align:right;"&gt; 1.00000 &lt;/td&gt;
  &lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
]
.pull-right[
![](Multiple_testing_files/figure-html/unnamed-chunk-12-1.png)&lt;!-- --&gt;
]
---

# Implications for our examples

- If we test multiple hypothesis simultaneously the overall type I error probability is not controlled anymore.

- Testing 12 tests simultaneosly yields almost a 50% chance of  a statistically significant result  __even if none of the hypothesis tested is false __

- How do we incorporate the impact of multiple testing on our inference?

---

# A simulated example (1)

- We simulated an omics study with 6000 genes whose expression has been measured on 8 cases and 8 controls, and where __no gene shows real difference between them__.

- What happens if we call _differentially expressed_ any gene with `\(p &lt; 0.05\)`

- The number of genes falsely rejected will be on average of `\(6000 \times \alpha\)`.
    
---

# We start with no differences ...

&lt;img src="images/simulatedArray1.png" width="120%" style="display: block; margin: auto;" /&gt;
---

# As more genes are checked ...

&lt;img src="images/simulatedArray2.png" width="110%" style="display: block; margin: auto;" /&gt;
---

# All together, sorted by p-values

&lt;img src="images/simulatedArray3.png" width="110%" style="display: block; margin: auto;" /&gt;
---

# So what can be done?

- Intuitive idea: Doing many tests increases the chances of calling false positives,

- This may be compensated 
    + Using _more restrictive error rates_, for instance 0.01 or 0.005 instead of 0.05.
    
    + Adjusting ("correcting") the p-values to compensate for the number of tests.
    
---

# Distinct Error Rates

- Individual error rate (IER)
    + Error rate of a single test.
    + For a test with 5% significance level the IER is 0.05
    
- Global Error Rate
    + Error rate for one or several groups of tests. 
    + For a group of tests each with 5% significance the global error rate is &gt; 5%

---

&lt;!-- # Extending type I error control --&gt;

&lt;!-- - With more than 1 test rejecting hypotheses with a p-value less than `\(\alpha\)` doesn’t control for `\(P[FD]\)` anymore. --&gt;
&lt;!-- - What can be done? --&gt;
&lt;!--      + Extend the idea of type I error --&gt;
&lt;!--           - FWER and FDR are two such extensions --&gt;

&lt;!--      + Look for procedures that control the probability for these extended error types --&gt;
&lt;!--           - Mainly adjust raw p-values --&gt;

&lt;!-- --- --&gt;


# Decision table for many tests

- With many tests we count discoveries 

&lt;img src="images/decisionTable2.png" width="100%" style="display: block; margin: auto;" /&gt;

---

# Two main error rate extensions

- Family Wise Error Rate (FWER) 
    + FWER is probability of at least 1 False Discovery
    + FWER = P (FD &gt; 0) 
   
- False Discovery Rate (FDR)
    + FDR is expected value of proportion of False Discoveries among all Discoveries .
    + FDR = E (FD/D; D&gt;0)
---

# FWER / FDR control procedures

- FWER 
    + Bonferroni
    + Holm (1979)
    + Hochberg (1986)

- FDR
    + Benjamini &amp; Hochberg (1995)
    + Benjamini &amp; Yekutieli (2001)
---

# Controlling the FWER (Bonferroni)

- Bonferroni procedure: Adjust significance level for number of tests performed (m)

  + Use significance level `\(\alpha/m\)`, 

- Equivalently, adjust p-values multiplying all p-values by m.

- Other, more efficient procedures available: See a statistician

---

# Example. Presenting data.

.pull-left[
.font70[

- García-Arenzana et al. (2014) tested associations of 25 dietary variables with mammographic density, an important risk factor for breast cancer, in Spanish women.
- They found the following results (only first 10 are shown)
]
]
.pull-right[
&lt;img src="images/dietExample1.png" width="100%" style="display: block; margin: auto;" /&gt;
]

[See complete example](http://www.biostathandbook.com/multiplecomparisons.html)

---

# Example. Bonferroni (FWER)

&lt;img src="images/dietExample2.png" width="100%" style="display: block; margin: auto;" /&gt;
---


# Controlling the FDR (B &amp; H)

- Benjamini-Hochberg procedure: Provides control of FDR for a fixed FDR value 
    + 5% FDR: On average, 5% of your significant findings will be false
    
- Important: FDR is not an individual error rate.
    + A number higher than 0.05, such as 0.10 or 0.25 can be used

---

# Benjamini &amp; Hochberg
    
- Procedure is relatively simple
    + Order the p-values
    + To provide control at a `\(Q\)` FDR value compare i-th smallest p-value to  `\(i \times Q/m\)`

- Instead of setting the FDR at a fixed value and establishing significance/non significance an, __adjusted p-value__ may be computed.

---

# Example. B-H (FDR)

&lt;img src="images/dietExample3.png" width="100%" style="display: block; margin: auto;" /&gt;
---

# Example: Adjustments with R

&lt;img src="images/MTAdjwithR.png" width="100%" style="display: block; margin: auto;" /&gt;
---

# What error rate to control for

- FWER Controls for no (0) false positives
     + Rejects many fewer hypothese (less false positives),
     + but you are likely to miss many.
     + Adequate if goal is to identify few cases that differ between two groups.
     
---

# What error rate to control for

- FDR Controls the (expected) proportion of false positives
    + if you can tolerate more false positives
    + you will get many fewer false negatives
    + Adequate if goal is to pursue the study e.g. to determine functional relationships among genes

---
# Should we adjust for MT?

- This a controversial issue.

- Many authors are in favour

.font70[
+ Moyé:“ _Type I error accumulates with each executed hypothesis test and must be controlled by the investigators_”
    
+ Blakesley et al. : “ _Failure to control type I errors when examining multiple outcomes may yield false inferences, which may slow or sidetrack research progress_”

]
---
# Or shouldn't we?

- Rothman: "No adjustments are needed for multiple comparisons"

.font70[

+ _Reducing the type I error for null associations increases the type II error for those associations that are not null_
    
+ _A policy of not making adjustments for multiple comparisons is preferable because it will lead to fewer errors of interpretation when the data under evaluation are not random numbers but actual observations on nature_.
    
+ _Scientists should not be so reluctant to explore leads that may turn out to be wrong that they penalize themselves by missing possibly important findings_. 
]

---

# Summary: Basic principles

1. The multiple comparisons problem (MCP) __should not be ignored__.

2. __Limiting the number of outcomes and subgroups__ is one of the best ways to address the MCP.

3. The MCP should be addressed by __first structuring the data__. &lt;br&gt;
Furthermore, protocols for addressing the MCP __should be made before data analysis is undertaken__.

---

# Developing a Strategy for MT

1. Delineate separate outcome domains in the study protocols.
2. Define confirmatory and exploratory analysis components prior to data analysis.
3. As a general rule consider adjusting for multiple testing in confirmatory analysis.
4. As a general rule exploratory analysis does not require adjusting for multiple testing.
5. Specify which subgroups will be part of the confirmatory analysis and which ones will be part of the exploratory analysis.

---

# Developing a Strategy (II)

&lt;ol start="6"&gt;
&lt;li&gt; Apply multiplicity adjustments in experimental designs with multiple treatment groups.&lt;/li&gt;
&lt;li&gt; Design the study to have sufficient statistical power for examining intervention effects for all prespecified confirmatory analyses.&lt;/li&gt;
&lt;li&gt; Qualify confirmatory and exploratory analysis findings in the study reports.
&lt;/li&gt;
&lt;/ol&gt;
---

# References

.font40[

1. Blakesley RE, Mazumdar S, Dew MA, et al. Comparisons of Methods for Multiple Hypothesis Testing in Neuropsychological Research. Neuropsychology. 2009;23(2):255-264. doi:10.1037/a0012850
2. Moyé LA. P-value interpretation and alpha allocation in clinical trials. Ann Epidemiol. 1998;8(6):351-357. doi:10.1016/s1047-2797(98)00003-9
3. Rothman KJ. No adjustments are needed for multiple comparisons. Epidemiology. 1990;1(1):43-46. doi:10.1097/00001648-199001000-00010
4. Streiner DL. Best (but oft-forgotten) practices: The multiple problems of multiplicity-whether and how to correct for many statistical tests. Am J Clin Nutr. 2015;102(4):721-728. doi:10.3945/ajcn.115.113548
5. View of Planned Hypothesis Tests Are Not Necessarily Exempt From Multiplicity Adjustment | Journal of Research Practice. http://jrp.icaap.org/index.php/jrp/article/view/514/417. Accessed November 25, 2019.
6. Asante I, Pei H, Zhou E, et al. Exploratory metabolomic study to identify blood-based biomarkers as a potential screen for colorectal cancer. Mol Omi. 2019;15(1):21-29. doi:10.1039/c8mo00158h
7. Matsui S. Confirmatory and Exploratory Analyses in Omics Studies with Particular Focus on Multiple Testing and &amp;lt;i&amp;gt;P&amp;lt;/i&amp;gt;-value. Japanese J Biometrics. 2018;38(2):127-139. doi:10.5691/jjb.38.127
8. Kimmelman J, Mogil JS, Dirnagl U. Distinguishing between Exploratory and Confirmatory Preclinical Research Will Improve Translation. PLoS Biol. 2014;12(5). doi:10.1371/journal.pbio.1001863
9. Sethuraman A, Gonzalez NM, Grenier CE, et al. Continued misuse of multiple testing correction methods in population genetics‐A wake‐up call? Mol Ecol Resour. 2019;19(1):23-26. doi:10.1111/1755-0998.12969
10. Benjamini Y, Yekutieli D. The control of the false discovery rate in multiple testing under dependency. Ann Stat. 2001;29(4):1165-1188. doi:10.1214/aos/1013699998
11. Benjamini Y, Hochberg Y. Controlling the False Discovery Rate: A Practical and Powerful Approach to Multiple Testing. J R Stat Soc Ser B. 1995;57(1):289-300. doi:10.1111/j.2517-6161.1995.tb02031.x
12. Hauser S, Wakeland K, Leberg P. Inconsistent use of multiple comparison corrections in studies of population genetic structure: Are some type I errors more tolerable than others? Mol Ecol Resour. 2019;19(1):144-148. doi:10.1111/1755-0998.12947
13. 10 Things to Know About Multiple Comparisons | Egap. https://egap.org/methods-guides/10-things-you-need-know-about-multiple-comparisons. Accessed November 9, 2019.
14. Konishi T. Microarray test results should not be compensated for multiplicity of gene contents. BMC Syst Biol. 2011;5(Suppl 2):S6. doi:10.1186/1752-0509-5-S2-S6

]
---

# QUESTIONS?

&lt;img src="images/questions.jpg" width="90%" style="display: block; margin: auto;" /&gt;
    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
